{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Carvana.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkalim/Deep-Learning/blob/master/Carvana.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKd7CZh63XUy",
        "colab_type": "text"
      },
      "source": [
        "# Carvana Image Masking Challenge (Semantic Segmentation)\n",
        "\n",
        "In this tutorial we will learn how to segment images. Segmentation is the process of generating pixel-wise segmentations giving the class of the object visible at each pixel. For example, we could be identifying the location and boundaries of people within an image or identifying cell nuclei from an image. Formally, image segmentation refers to the process of partitioning an image into a set of pixels that we desire to identify (our target) and the background.\n",
        "\n",
        "Specifically, in this tutorial we will be using the Carvana Image Masking Challenge Dataset.\n",
        "\n",
        "This dataset contains a large number of car images, with each car taken from different angles. In addition, for each car image, we have an associated manually cutout mask; our task will be to automatically create these cutout masks for unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VccMYEdLrO-_",
        "colab_type": "text"
      },
      "source": [
        "## Mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqNXAqINQXDv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e388f891-c0e3-411b-ea6e-bfb019a5f15f"
      },
      "source": [
        "### Mount the Drive ###\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtPF2Sq4fSOI",
        "colab_type": "text"
      },
      "source": [
        "## Constants\n",
        "*Note* that due to the architecture of our UNet version, the size of the image must be evenly divisible by a factor of 32, as we down sample the spatial resolution by a factor of 2 with each MaxPooling2Dlayer.\n",
        "\n",
        "Note that lowering the image resolution will decrease performance and lowering batch size will increase training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImfQIq5GZVTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_shape = (256, 256, 3)\n",
        "batch_size = 16\n",
        "epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR9igf6tQmMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import zipfile\n",
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "mpl.rcParams['figure.figsize'] = (12,12)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib as tfcontrib\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import backend as K  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bpy5n3NbRuU",
        "colab_type": "text"
      },
      "source": [
        "## Download Carvana dataset from Kaggle\n",
        "You must accept the rules before downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj8c7rFkQmJa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "95cbff85-0c28-49ce-99cc-edcea0518906"
      },
      "source": [
        "!pip install kaggle\n",
        "\n",
        "### Go to my account on Kaggle website and generate new API token ###\n",
        "### API token will be downloaded as kaggle.json file ###\n",
        "\n",
        "# Upload the API token.\n",
        "def get_kaggle_credentials():\n",
        "  token_dir = os.path.join(os.path.expanduser(\"~\"),\".kaggle\")\n",
        "  token_file = os.path.join(token_dir, \"kaggle.json\")\n",
        "  if not os.path.isdir(token_dir):\n",
        "    os.mkdir(token_dir)\n",
        "  try:\n",
        "    with open(token_file,'r') as f:\n",
        "      pass\n",
        "  except IOError as no_file:\n",
        "    try:\n",
        "      from google.colab import files\n",
        "    except ImportError:\n",
        "      raise no_file\n",
        "    \n",
        "    uploaded = files.upload()\n",
        "    \n",
        "    if \"kaggle.json\" not in uploaded:\n",
        "      raise ValueError(\"You need an API key! see: \"\n",
        "                       \"https://github.com/Kaggle/kaggle-api#api-credentials\")\n",
        "    with open(token_file, \"wb\") as f:\n",
        "      f.write(uploaded[\"kaggle.json\"])\n",
        "    os.chmod(token_file, 600)\n",
        "\n",
        "get_kaggle_credentials()\n",
        "\n",
        "import kaggle"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lal8YGD_c79C",
        "colab_type": "text"
      },
      "source": [
        " #### Download data from Kaggle and unzip the files of interest. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMfLHxl5QmGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function will extract the desired zip files in the dataset\n",
        "def load_data_from_zip(competition, file):\n",
        "  with zipfile.ZipFile(os.path.join(competition, file), \"r\") as zip_ref:\n",
        "    unzipped_file = zip_ref.namelist()[0]\n",
        "    zip_ref.extractall(competition)\n",
        "\n",
        "# This function will download the necessary files from the dataset and use the above function to unzip them    \n",
        "def get_data(competition):\n",
        "    kaggle.api.competition_download_files(competition, competition)\n",
        "    load_data_from_zip(competition, 'train.zip')\n",
        "    load_data_from_zip(competition, 'train_masks.zip')\n",
        "    load_data_from_zip(competition, 'train_masks.csv.zip')\n",
        "    \n",
        "\n",
        "# You must accept the competition rules before downloading\n",
        "competition_name = 'carvana-image-masking-challenge'\n",
        "\n",
        "# Download the dataset    \n",
        "get_data(competition_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtZi8r4edVJA",
        "colab_type": "text"
      },
      "source": [
        "#### Process the filenames (paths)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOgtr51TQmFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_dir = os.path.join(competition_name, \"train\")\n",
        "label_dir = os.path.join(competition_name, \"train_masks\")\n",
        "\n",
        "df_train = pd.read_csv(os.path.join(competition_name, 'train_masks.csv'))\n",
        "ids_train = df_train['img'].map(lambda s: s.split('.')[0])\n",
        "\n",
        "x_train_filenames = []\n",
        "y_train_filenames = []\n",
        "for img_id in ids_train:\n",
        "  x_train_filenames.append(os.path.join(img_dir, \"{}.jpg\".format(img_id)))\n",
        "  y_train_filenames.append(os.path.join(label_dir, \"{}_mask.gif\".format(img_id)))\n",
        "    \n",
        "x_train_filenames, x_val_filenames, y_train_filenames, y_val_filenames = \\\n",
        "                    train_test_split(x_train_filenames, y_train_filenames, test_size=0.2, random_state=42)\n",
        "\n",
        "num_train_examples = len(x_train_filenames)\n",
        "num_val_examples = len(x_val_filenames)\n",
        "\n",
        "print(\"Number of training examples: {}\".format(num_train_examples))\n",
        "print(\"Number of validation examples: {}\".format(num_val_examples))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5V952_3QmCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here is what the filenames look like\n",
        "x_train_filenames[:10]\n",
        "y_train_filenames[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgMKHReid6iG",
        "colab_type": "text"
      },
      "source": [
        "### Visualise the dataset\n",
        "Here is how the images and masks in the dataset look like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eAhLgXRQl-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of items to be displayed\n",
        "display_num = 5\n",
        "\n",
        "# Randomly selecting the examples from the dataset\n",
        "r_choices = np.random.choice(num_train_examples, display_num)\n",
        "\n",
        "plt.figure(figsize=(10, 15))\n",
        "for i in range(0, display_num * 2, 2):\n",
        "  img_num = r_choices[i // 2]\n",
        "\n",
        "  x_pathname = x_train_filenames[img_num]\n",
        "  y_pathname = y_train_filenames[img_num]\n",
        "  \n",
        "  plt.subplot(display_num, 2, i + 1)\n",
        "  plt.imshow(mpimg.imread(x_pathname))\n",
        "  plt.title(\"Original Image\")\n",
        "  \n",
        "  plt.subplot(display_num, 2, i + 2)\n",
        "  plt.imshow(Image.open(y_pathname))              # Image.open() is required for the colormap\n",
        "  plt.title(\"Masked Image\")  \n",
        "  \n",
        "plt.suptitle(\"Examples of Images and their Masks\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytW2udA2eyat",
        "colab_type": "text"
      },
      "source": [
        "### Data Augmentation\n",
        "\n",
        "Data augmentation \"increases\" the amount of training data by augmenting them via a number of random transformations. During training time, our model would never see twice the exact same picture. This helps prevent overfitting and helps the model generalize better to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuTTjtDQQl86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function processes each pathname (image path)\n",
        "\n",
        "def _process_pathnames(fname, label_path):\n",
        "  # We map this function onto each pathname pair  \n",
        "  img_str = tf.read_file(fname)\n",
        "  img = tf.image.decode_jpeg(img_str, channels=3)\n",
        "\n",
        "  label_img_str = tf.read_file(label_path)\n",
        "  # These are gif images so they return as (num_frames, h, w, c)\n",
        "  label_img = tf.image.decode_gif(label_img_str)[0]\n",
        "  # The label image should only have values of 1 or 0, indicating pixel wise\n",
        "  # object (car) or not (background). We take the first channel only. \n",
        "  label_img = label_img[:, :, 0]\n",
        "  label_img = tf.expand_dims(label_img, axis=-1)\n",
        "  return img, label_img\n",
        "\n",
        "# Shifting the image\n",
        "# width_shift_range and height_shift_range are ranges \n",
        "# (as a fraction of total width or height) within which to randomly \n",
        "# translate the image either horizontally or vertically. \n",
        "# This transformation must be applied to both the label and the actual image.\n",
        "\n",
        "def shift_img(output_img, label_img, width_shift_range, height_shift_range):\n",
        "  \"\"\"This fn will perform the horizontal or vertical shift\"\"\"\n",
        "  if width_shift_range or height_shift_range:\n",
        "      if width_shift_range:\n",
        "        width_shift_range = tf.random_uniform([], \n",
        "                                              -width_shift_range * img_shape[1],\n",
        "                                              width_shift_range * img_shape[1])\n",
        "      if height_shift_range:\n",
        "        height_shift_range = tf.random_uniform([],\n",
        "                                               -height_shift_range * img_shape[0],\n",
        "                                               height_shift_range * img_shape[0])\n",
        "      # Translate both \n",
        "      output_img = tfcontrib.image.translate(output_img,\n",
        "                                             [width_shift_range, height_shift_range])\n",
        "      label_img = tfcontrib.image.translate(label_img,\n",
        "                                             [width_shift_range, height_shift_range])\n",
        "  return output_img, label_img\n",
        "\n",
        "\n",
        "# Flipping the image horizontally\n",
        "# flip the image horizontally along the central axis with a 0.5 probability. \n",
        "# This transformation must be applied to both the label and the actual image.\n",
        "\n",
        "def flip_img(horizontal_flip, tr_img, label_img):\n",
        "  if horizontal_flip:\n",
        "    flip_prob = tf.random_uniform([], 0.0, 1.0)\n",
        "    tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n",
        "                                lambda: (tf.image.flip_left_right(tr_img), tf.image.flip_left_right(label_img)),\n",
        "                                lambda: (tr_img, label_img))\n",
        "  return tr_img, label_img\n",
        "\n",
        "\n",
        "# Function that performs all the required augmentations\n",
        "def _augment(img,\n",
        "             label_img,\n",
        "             resize=None,  # Resize the image to some size e.g. [256, 256]\n",
        "             scale=1,  # Scale image e.g. 1 / 255.\n",
        "             hue_delta=0,  # Adjust the hue of an RGB image by random factor\n",
        "             horizontal_flip=False,  # Random left right flip,\n",
        "             width_shift_range=0,  # Randomly translate the image horizontally\n",
        "             height_shift_range=0):  # Randomly translate the image vertically \n",
        "    \n",
        "  if resize is not None:\n",
        "    # Resize both images\n",
        "    label_img = tf.image.resize_images(label_img, resize)\n",
        "    img = tf.image.resize_images(img, resize)\n",
        "  \n",
        "  # hue_delta - Adjusts the hue of an RGB image by a random factor. \n",
        "  # This is only applied to the actual image (not our label image). \n",
        "  # The hue_delta must be in the interval [0, 0.5]\n",
        "  if hue_delta:\n",
        "    img = tf.image.random_hue(img, hue_delta)\n",
        "  \n",
        "  img, label_img = flip_img(horizontal_flip, img, label_img)\n",
        "  img, label_img = shift_img(img, label_img, width_shift_range, height_shift_range)\n",
        "  label_img = tf.to_float(label_img) * scale\n",
        "  img = tf.to_float(img) * scale \n",
        "  return img, label_img\n",
        "\n",
        "# Function to create a dataset after data augmentation\n",
        "def get_baseline_dataset(filenames, \n",
        "                         labels,\n",
        "                         preproc_fn=functools.partial(_augment),\n",
        "                         threads=5, \n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True):           \n",
        "  num_x = len(filenames)\n",
        "\n",
        "  # Create a dataset from the filenames and labels\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "\n",
        "  # Map our preprocessing function to every element in our dataset, taking advantage of multithreading\n",
        "  dataset = dataset.map(_process_pathnames, num_parallel_calls=threads)\n",
        "  if preproc_fn.keywords is not None and 'resize' not in preproc_fn.keywords:\n",
        "    assert batch_size == 1, \"Batching images must be of the same size\"\n",
        "\n",
        "  dataset = dataset.map(preproc_fn, num_parallel_calls=threads)\n",
        "  \n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(num_x)\n",
        "  \n",
        "  \n",
        "  # It's necessary to repeat our data for all epochs \n",
        "  dataset = dataset.repeat().batch(batch_size)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi78d0UhijDP",
        "colab_type": "text"
      },
      "source": [
        "### Create training and validation datasets\n",
        "#### Data augmentation is applied only to training data and not to validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_AvXXORXDuC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "c4af578c-fa1d-4a83-dacc-9d30539b1f3c"
      },
      "source": [
        "# augmentation configuration for training data\n",
        "tr_cfg = {\n",
        "    'resize': [img_shape[0], img_shape[1]],\n",
        "    'scale': 1 / 255.,\n",
        "    'hue_delta': 0.1,\n",
        "    'horizontal_flip': True,\n",
        "    'width_shift_range': 0.1,\n",
        "    'height_shift_range': 0.1\n",
        "}\n",
        "tr_preprocessing_fn = functools.partial(_augment, **tr_cfg)\n",
        "\n",
        "# augmentation configuration for validation data\n",
        "val_cfg = {\n",
        "    'resize': [img_shape[0], img_shape[1]],\n",
        "    'scale': 1 / 255.,\n",
        "}\n",
        "val_preprocessing_fn = functools.partial(_augment, **val_cfg)\n",
        "\n",
        "# Prepare training dataset\n",
        "train_ds = get_baseline_dataset(x_train_filenames,\n",
        "                                y_train_filenames,\n",
        "                                preproc_fn=tr_preprocessing_fn,\n",
        "                                batch_size=batch_size)\n",
        "\n",
        "# Prepare validation dataset\n",
        "val_ds = get_baseline_dataset(x_val_filenames,\n",
        "                              y_val_filenames, \n",
        "                              preproc_fn=val_preprocessing_fn,\n",
        "                              batch_size=batch_size)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-9-c158537256c1>:77: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSpgtKNmmEcm",
        "colab_type": "text"
      },
      "source": [
        "## Testing the image augmentor data pipeline\n",
        "Plot the augmented data and its corresponding mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2QLHASMXTNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare a temporary dataset belonging to training data with a batch size of 1\n",
        "temp_ds = get_baseline_dataset(x_train_filenames, \n",
        "                               y_train_filenames,\n",
        "                               preproc_fn=tr_preprocessing_fn,\n",
        "                               batch_size=1,\n",
        "                               shuffle=False)\n",
        "\n",
        "# Let's examine some of these augmented images\n",
        "data_aug_iter = temp_ds.make_one_shot_iterator()\n",
        "next_element = data_aug_iter.get_next()\n",
        "\n",
        "# Running next element in our graph will produce an image since batch size = 1\n",
        "with tf.Session() as sess: \n",
        "  batch_of_imgs, label = sess.run(next_element)\n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  img = batch_of_imgs[0]\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(img)\n",
        "  plt.title(\"Original Image\")\n",
        "    \n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(label[0, :, :, 0])\n",
        "  plt.title(\"Masked Image\")\n",
        "\n",
        "  plt.show()\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmV68yykYbo2",
        "colab_type": "text"
      },
      "source": [
        "## Defining the model (UNET)\n",
        "U-Net is especially good with segmentation tasks because it can localize well to provide high resolution segmentation masks. In addition, it works well with small datasets and is relatively robust against overfitting as the training data is in terms of the number of patches within an image, which is much larger than the number of training images itself. Unlike the original model, we will add batch normalization to each of our blocks.\n",
        "\n",
        "The Unet is built with an encoder portion and a decoder portion. The encoder portion is composed of a linear stack of Conv, BatchNorm, and Relu operations followed by a MaxPool. Each MaxPool will reduce the spatial resolution of our feature map by a factor of 2. We keep track of the outputs of each block as we feed these high resolution feature maps with the decoder portion. The Decoder portion is comprised of UpSampling2D, Conv, BatchNorm, and Relus. Note that we concatenate the feature map of the same size on the decoder side. Finally, we add a final Conv operation that performs a convolution along the channels for each individual pixel (kernel size of (1, 1)) that outputs our final segmentation mask in grayscale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eqAlZRCXTKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defing the convolutional block\n",
        "def conv_block(input_tensor, num_filters):\n",
        "  encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "  encoder = layers.BatchNormalization()(encoder)\n",
        "  encoder = layers.Activation('relu')(encoder)\n",
        "  encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "  encoder = layers.BatchNormalization()(encoder)\n",
        "  encoder = layers.Activation('relu')(encoder)\n",
        "  return encoder\n",
        "\n",
        "# Defining encoder block\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "  encoder = conv_block(input_tensor, num_filters)\n",
        "  encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "  return encoder_pool, encoder\n",
        "\n",
        "# Defining decoder block\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "  decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "  decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "  decoder = layers.BatchNormalization()(decoder)\n",
        "  decoder = layers.Activation('relu')(decoder)\n",
        "  decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "  decoder = layers.BatchNormalization()(decoder)\n",
        "  decoder = layers.Activation('relu')(decoder)\n",
        "  decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "  decoder = layers.BatchNormalization()(decoder)\n",
        "  decoder = layers.Activation('relu')(decoder)\n",
        "  return decoder\n",
        "\n",
        "inputs = layers.Input(shape=img_shape)\n",
        "\n",
        "encoder0_pool, encoder0 = encoder_block(inputs, 32)\n",
        "\n",
        "encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64)\n",
        "\n",
        "encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128)\n",
        "\n",
        "encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256)\n",
        "\n",
        "encoder4_pool, encoder4 = encoder_block(encoder3_pool, 512)\n",
        "\n",
        "center = conv_block(encoder4_pool, 1024)\n",
        "\n",
        "decoder4 = decoder_block(center, encoder4, 512)\n",
        "\n",
        "decoder3 = decoder_block(decoder4, encoder3, 256)\n",
        "\n",
        "decoder2 = decoder_block(decoder3, encoder2, 128)\n",
        "\n",
        "decoder1 = decoder_block(decoder2, encoder1, 64)\n",
        "\n",
        "decoder0 = decoder_block(decoder1, encoder0, 32)\n",
        "\n",
        "outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)\n",
        "\n",
        "# using keras functional API for defining model\n",
        "model = models.Model(inputs=[inputs], outputs=[outputs])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtjqemh8YoYD",
        "colab_type": "text"
      },
      "source": [
        "## Defining custom loss function\n",
        "Dice loss is a metric that measures overlap. We use dice loss here because it performs better at class imbalanced problems by design. In addition, maximizing the dice coefficient and IoU metrics are the actual objectives and goals of our segmentation task. Using cross entropy is more of a proxy which is easier to maximize. Instead, we maximize our objective directly.\n",
        "\n",
        "Here, we'll use a specialized loss function that combines binary cross entropy and our dice loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0I11AmJXs9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dice_coeff(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    # Flatten\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
        "    return score\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "    return loss\n",
        "\n",
        "# loss function that combines binary cross entropy and dice loss\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyjmW97roEVG",
        "colab_type": "text"
      },
      "source": [
        "## Compiling and training the model the model with callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMTaP0GaXs6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_loss])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "save_model_path = \"/content/drive/My Drive/AI/DL/Image Segmentation/carvana1.hdf5\"\n",
        "\n",
        "cp = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_path, monitor='val_dice_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "history = model.fit(train_ds,               # training dataset\n",
        "                   steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))),\n",
        "                   epochs=epochs,\n",
        "                   validation_data=val_ds,\n",
        "                   validation_steps=int(np.ceil(num_val_examples / float(batch_size))),\n",
        "                   callbacks=[cp])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xNxND5dowP3",
        "colab_type": "text"
      },
      "source": [
        "## Training and Validation Loss Curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9iCZj17Xs0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dice = history.history['dice_loss']\n",
        "val_dice = history.history['val_dice_loss']\n",
        "\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, train_dice, label='Training Dice Loss')\n",
        "plt.plot(epochs_range, val_dice, label='Validation Dice Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Dice Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, train_loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXGKaewOo7PN",
        "colab_type": "text"
      },
      "source": [
        "## Loading model with pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_WwvpwEXsyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Alternatively, load the weights directly: model.load_weights(save_model_path)\n",
        "model = models.load_model(save_model_path, custom_objects={'bce_dice_loss': bce_dice_loss,'dice_loss': dice_loss})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjIZoZlCpRtO",
        "colab_type": "text"
      },
      "source": [
        "## Predict\n",
        "### Visualise the results of the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAx_n2PUXswt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_result = 10\n",
        "\n",
        "# Let's visualize some of the outputs \n",
        "data_aug_iter = val_ds.make_one_shot_iterator()\n",
        "next_element = data_aug_iter.get_next()\n",
        "\n",
        "# Running next element in our graph will produce a batch of images\n",
        "plt.figure(figsize=(10, 20))\n",
        "for i in range(num_result):\n",
        "  batch_of_imgs, label = tf.keras.backend.get_session().run(next_element)\n",
        "  img = batch_of_imgs[0]\n",
        "  predicted_label = model.predict(batch_of_imgs)[0]\n",
        "\n",
        "  plt.subplot(10, 3, 3 * i + 1)\n",
        "  plt.imshow(img)\n",
        "  plt.title(\"Input image\")\n",
        "  \n",
        "  plt.subplot(10, 3, 3 * i + 2)\n",
        "  plt.imshow(label[0, :, :, 0])\n",
        "  plt.title(\"Actual Mask\")\n",
        "    \n",
        "  plt.subplot(10, 3, 3 * i + 3)\n",
        "  plt.imshow(predicted_label[:, :, 0])\n",
        "  plt.title(\"Predicted Mask\")\n",
        "\n",
        "plt.suptitle(\"Examples of Input Image, Label, and Prediction\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA4SGwyu2LQQ",
        "colab_type": "text"
      },
      "source": [
        "## Prediction on custom images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tBZcH9Z2L1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "image_dim = 256\n",
        "\n",
        "image = mpimg.imread(x_pathname)\n",
        "\n",
        "image = cv2.resize(image , (image_dim,image_dim))\n",
        "image = image.astype('float32')\n",
        "image = image/255\n",
        "image = np.expand_dims(image, axis=0)\n",
        "\n",
        "prediction = model.predict(image)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(image[0])\n",
        "plt.title(\"Input image\")\n",
        "  \n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(prediction[0][:, :, 0])\n",
        "plt.title(\"Predicted Mask\")\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}