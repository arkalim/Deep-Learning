{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. The surface is described using a grid like the following:\n",
    "\n",
    "                                                       SFFF\n",
    "                                                       FHFH\n",
    "                                                       FFFH\n",
    "                                                       HFFG\n",
    "                                                       \n",
    "This grid is our environment where S is the agent’s starting point, and it’s safe. F represents the frozen surface and is also safe. H represents a hole, and if our agent steps in a hole in the middle of a frozen lake, well, that’s not good. Finally, G represents the goal, which is the space on the grid where the prized frisbee is located.\n",
    "\n",
    "The agent can navigate left, right, up, and down, and the episode ends when the agent reaches the goal or falls in a hole. It receives a reward of one if it reaches the goal, and zero otherwise.                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing Necessary Packages \n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Description\n",
    "\n",
    "##### Learning Rate(α): \n",
    "The learning rate is a number between 0 and 1, which can be thought of as how quickly the agent abandons the previous Q-value for the new Q-value.\n",
    "\n",
    "#### Discount Rate(γ): \n",
    "It is a value betwen 0 and 1 which describes the rate at which we discount the future rewards in the expected return. This definition of the discounted return makes it to where our agent will care more about the immediate reward over future rewards since future rewards will be more heavily discounted. So, while the agent does consider the rewards it expects to receive in the future, the more immediate rewards have more influence when it comes to the agent making a decision about taking a particular action.\n",
    "\n",
    "#### Exploration rate(ϵ): \n",
    "It the probability that the agent will explore the environment rather than exploit its current information about the environment. It is initially set to 1 as the agent has no information when the game starts. As the agent learns more about the environment, at the start of each new episode, ϵ will decay by some rate that we set so that the likelihood of exploration becomes less and less probable as the agent learns more and more about the environment. This is called the epsilon greedy strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the game environment\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "# Specifying the action space and state space sizes\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "# Initialising the Q-table with 0 \n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(q_table);\n",
    "\n",
    "# No. of episodes we want our agent to play\n",
    "num_episodes = 10000\n",
    "\n",
    "# Here, we define the maximum number of steps that our agent is allowed to take within a single episode\n",
    "# If the agent fails to terminate the episode, it will auto terminate\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "# Learning Rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Discount Rate\n",
    "discount_rate = 0.99\n",
    "\n",
    "# Initial Exploration Rate\n",
    "exploration_rate = 1\n",
    "\n",
    "# Maximum Exploration Rate\n",
    "max_exploration_rate = 1\n",
    "\n",
    "# Maximum Exploration Rate\n",
    "min_exploration_rate = 0.01\n",
    "\n",
    "# The rate of decay of exploration rate\n",
    "exploration_decay_rate = 0.001\n",
    "\n",
    "# List to store the reward in each episode. This will be so we can see how our game score changes over time.\n",
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Initialize new episode:\n",
    "    #Resest the environment\n",
    "    state = env.reset()\n",
    "    \n",
    "    #The done variable just keeps track of whether or not our episode is finished\n",
    "    done = False\n",
    "    \n",
    "    #Reset the reward in the current episode to 0\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode): \n",
    "        \n",
    "        # Exploration-exploitation trade-off\n",
    "        # Generating a random number between 0 and 1\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        \n",
    "        # If the generated number is greater than the exploration rate, we exploit\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "            \n",
    "        # Else, we explore randomly    \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Now that our action is chosen, we then take that action by calling step() on our env object \n",
    "        # and passing our action to it. The function step() returns a tuple containing the new state, \n",
    "        # the reward for the action we took, whether or not the action ended our episode, and diagnostic \n",
    "        # information regarding our environment, which may be helpful for us if we end up needing to do any debugging.    \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "    \n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        # Next, we set our current state to the new_state that was returned to us once we took our last action\n",
    "        state=new_state\n",
    "        # we then update the rewards from our current episode by adding the reward we received for our previous action.\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        # If the episode is terminated, break the loop\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Once an episode is finished, we need to update our exploration_rate using exponential decay\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    # We then just append the rewards from the current episode to the list of rewards from all episodes\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "    # We’re good to move on to the next episode.\n",
    "    \n",
    "print(\"done\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"Average reward per thousand episodes:\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000\n",
    "    \n",
    "\n",
    "# Print updated Q-table\n",
    "print(\"\\n\\nQ-table\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch our agent play Frozen Lake \n",
    "\n",
    "for episode in range(100):\n",
    "    \n",
    "    # Initialize new episode:\n",
    "    #Resest the environment\n",
    "    state = env.reset()\n",
    "    \n",
    "    # The done variable just keeps track of whether or not our episode is finished\n",
    "    done = False\n",
    "    \n",
    "    # Print the number of episodes played\n",
    "    print(\"EPISODE \", episode+1, \"\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):    \n",
    "        \n",
    "        # Clear the output window, it waits to clear the output until there is another printout to prevent overwriting.\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # We then call render() on our env object, which will render the current state of the environment to the display\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        # Now that our agent has learnt how to play, we follow the optimum policy (exploitation)\n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        \n",
    "        # We now take the action by calling step() on our env object\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # If the episode terminated\n",
    "        if done:\n",
    "            \n",
    "            # Clear the output window, it waits to clear the output until there is another printout to prevent overwriting.\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # We then call render() on our env object, which will render the current state of the environment to the display\n",
    "            env.render()\n",
    "            \n",
    "            # If the reward in the current step is 1\n",
    "            if reward == 1:\n",
    "                print(\"You reached the goal!\")\n",
    "                time.sleep(1)\n",
    "                \n",
    "            else:\n",
    "                print(\"You fell through a hole!\")\n",
    "                time.sleep(1)\n",
    "            # Clear output again    \n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "            \n",
    "        # Set new state\n",
    "        state = new_state\n",
    "        \n",
    "env.close()  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
